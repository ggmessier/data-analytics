{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Chronic Homelessness Prediction Algorithms using Client Histories\n",
    "This notebook compares algorithms for predicting chronic homelessness using both classification metrics and an examination of the shelter access histories of the cohorts selected as chronic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Copyright (C) 2021 Geoffrey Guy Messier\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, copy, imp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "%aimport di_data\n",
    "from di_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirStr = '~/data/plwh/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess():\n",
    "    \n",
    "    tblAll = pd.read_hdf(dirStr + 'UniversityExportAnonymized.hd5')\n",
    "    \n",
    "    tblAll = tblAll[tblAll.Date >= pd.to_datetime('2007-07-01')]\n",
    "    \n",
    "    print('Total Entries: {}'.format(len(tblAll.index)))\n",
    "    print('Dates: ',min(tblAll.Date),' to ',max(tblAll.Date))\n",
    "\n",
    "    tbl = copy.deepcopy(tblAll[ [ 'ClientId', 'Date', 'EntryType', 'Age' ] ])\n",
    "    tbl['Police'] = (tblAll.PoliceLogFlag == 1) | (tblAll.CPS > 0)\n",
    "    tbl['Ems'] = (tblAll.EmsLogFlag == 1) | (tblAll.EMS > 0)\n",
    "    tbl['Health'] = (tblAll.Health > 0) | (tblAll.PhysicalHealth > 0) | (tblAll.MentalHealth > 0) | (tblAll.Medication > 0)\n",
    "    tbl['Violence'] = (tblAll.PhysicalViolence > 0) | (tblAll.Weapon > 0) | (tblAll.Spray > 0) | (tblAll.Brawl > 0) | (tblAll.Gun > 0) | (tblAll.Knife > 0)\n",
    "    tbl['Addiction'] = (tblAll.Addiction > 0) | (tblAll.Overdose > 0)    \n",
    "    \n",
    "    # To address left censoring: Remove all clients with first sleep date within a year of the 2008 data import.\n",
    "    leftStart = tbl.Date.min()\n",
    "    leftEnd = pd.to_datetime('2009-07-01')\n",
    "    \n",
    "    # To address right censoring: Remove all clients with first sleep date within approximately 2 years of the end\n",
    "    # of the data.  Reasoning: We want to allow a 2 year window to give the clients a chance to become chronic.\n",
    "    rightStart = pd.to_datetime('2018-01-20')\n",
    "    rightEnd = tbl.Date.max()\n",
    "    \n",
    "    nClientsAll = len(tbl.ClientId.unique())\n",
    "    \n",
    "    tbl = RemoveByStartDate(tbl,leftStart,leftEnd,tbl.EntryType == 'Sleep')\n",
    "    nLeftRemoved = nClientsAll - len(tbl.ClientId.unique())\n",
    "\n",
    "    tbl = RemoveByStartDate(tbl,rightStart,rightEnd,tbl.EntryType == 'Sleep')\n",
    "    nRightRemoved = nClientsAll - nLeftRemoved - len(tbl.ClientId.unique())\n",
    "\n",
    "    \n",
    "    # Discard all data before September 1, 2008 due to import errors from previous database.\n",
    "    tbl = tbl.loc[tbl.Date >= pd.to_datetime('2008-09-01')]\n",
    "\n",
    "    nClients = len(tbl.ClientId.unique())\n",
    " \n",
    "    print('Total Clients: {:d}/{:d} ({:.1f}%) ({:d} removed left, {:d} removed right)'\n",
    "          .format(nClients,nClientsAll,100.0*nClients/nClientsAll,nLeftRemoved,nRightRemoved))\n",
    "\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries: 5431521\n",
      "Dates:  2007-07-01 00:00:00  to  2020-01-20 00:00:00\n",
      "Total Clients: 18398/41935 (43.9%) (19967 removed left, 3570 removed right)\n"
     ]
    }
   ],
   "source": [
    "tbl = PreProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Chronic Shelter Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a timeline of stays for each client in order to determine who satisfies the Canadian federal chronic shelter use definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateStayTimelines():\n",
    "    return tbl.loc[tbl.EntryType=='Sleep'].groupby('ClientId').progress_apply(CalculateStaySequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30172e9d7f544b79abb402ef1c14e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tlSty = GenerateStayTimelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies a time windowed threshold test to a count of stays.\n",
    "def TimeToCdnFedChronic(tbl):\n",
    "    \n",
    "    # First Test: 180 days in past 1 year\n",
    "    winSz = 365\n",
    "    thresh = 180\n",
    "    \n",
    "    win = tbl.rolling('%dd' % winSz,on='Date').count().Ind\n",
    "    registrationDate = tbl.Date.min()\n",
    "    idDate1 = tbl[win >= thresh].Date.min()  # Will be equal to NaN if the threshold isn't met.\n",
    "    \n",
    "    # Second Test: 546 days in past 3 years\n",
    "    winSz = 365*3\n",
    "    thresh = 546\n",
    "    \n",
    "    win = tbl.rolling('%dd' % winSz,on='Date').count().Ind\n",
    "    registrationDate = tbl.Date.min()\n",
    "    idDate2 = tbl[win >= thresh].Date.min()  # Will be equal to NaN if the threshold isn't met.\n",
    "    \n",
    "    idDate = min([ idDate1, idDate2 ])\n",
    "    \n",
    "    if idDate == idDate:   # Satisfied if idDate is not NaN.\n",
    "        return pd.Series({\n",
    "            'Flag': 'chr',  # Flag indicating test was satisfied.\n",
    "            'Date': idDate,   # Date client was identified.\n",
    "            'Time': (idDate - registrationDate).days + 1 # Number of days it took to identify client.\n",
    "        })\n",
    "    else:\n",
    "        return pd.Series({   # Returned if the test is not satisfied.\n",
    "            'Flag': 'tmp',\n",
    "            'Date': tbl.Date.max(),\n",
    "            'Time': (tbl.Date.max()-tbl.Date.min()).days + 1\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CdnFedChronicTte():\n",
    "    return tlSty.groupby('ClientId').progress_apply(TimeToCdnFedChronic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4574351a54a3cb4164c7565c34e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tte = CdnFedChronicTte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chronic clients: 1549/18398 (8.4%)\n"
     ]
    }
   ],
   "source": [
    "nChron = sum(tte.Flag == 'chr')\n",
    "nClients = len(tte.Flag)\n",
    "print('Chronic clients: {}/{} ({:.1f}%)'.format(nChron,nClients,100.0*nChron/nClients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Points\n",
    "- The time points after first sleep date where the chronic predictions are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTimes = [ 90 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data Features\n",
    "- These data features are the sum of the individual events in a client's data record (ie. sleeps, bars, record with a violence word, etc.) over a period starting at client's first sleep check-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummarizeEntries(tbl,sleepDateDelta):\n",
    "\n",
    "    endDate = tbl.loc[tbl.EntryType=='Sleep'].Date.min() + sleepDateDelta\n",
    "    tblValid = tbl.loc[tbl.Date <= endDate]\n",
    "    \n",
    "    entryCountsAndAge = pd.Series({        \n",
    "        'Age': max([ 0, tblValid.Age.max() ]),\n",
    "        'Bar': sum(tblValid.EntryType == 'Bar'),\n",
    "        'Counsellor': sum(tblValid.EntryType == 'CounsellorsNotes')+sum(tblValid.EntryType == 'ProgressDetails'),\n",
    "        'Log': sum(tblValid.EntryType == 'Log'),\n",
    "        'Sleep': sum(tblValid.EntryType == 'Sleep'),\n",
    "        })\n",
    "    \n",
    "    return pd.concat([ entryCountsAndAge, tblValid[['Police','Ems','Health','Violence','Addiction']].sum() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateFeatureTable(tbl,assmtPnts):\n",
    "    \n",
    "    deltas = { '{}d'.format(pt): pd.Timedelta(pt,unit='d') for pt in assmtPnts }\n",
    "    \n",
    "    colIndNames = [ \n",
    "        deltas.keys(), \n",
    "        ['Age','Bar','Counsellor','Log','Sleep','Police','Ems','Health','Violence','Addiction']\n",
    "    ]\n",
    "\n",
    "    tbls = {}\n",
    "    \n",
    "    for winStr in deltas.keys():\n",
    "\n",
    "        startDate = tbl[tbl.EntryType=='Sleep'].Date.min()\n",
    "        endDate = startDate + deltas[winStr]\n",
    "\n",
    "        tbls[winStr] = tbl.groupby('ClientId').progress_apply(SummarizeEntries,sleepDateDelta=deltas[winStr])\n",
    "        \n",
    "    return pd.concat(tbls.values(),axis=1,keys=tbls.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb6448f77674c32b8ca4fccd4915449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datX = GenerateFeatureTable(tbl,assmtPnts = predictionTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateClientDemographics():\n",
    "    return tbl.groupby('ClientId').progress_apply(ShelterGroupDemographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ed215cfd4a46ab88f0899ab189decf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demog = CalculateClientDemographics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies a time windowed threshold test to a count of stays.\n",
    "def SleepThreshChronicTest(tbl,threshPct,winSizes):\n",
    "        \n",
    "    results = []\n",
    "    for winSz in winSizes:\n",
    "        thresh = int(winSz*threshPct)\n",
    "        win = tbl.rolling('%dd' % winSz,on='Date').count().Ind\n",
    "        idDate = tbl[win >= thresh].Date.min()  # Will be equal to NaN if the threshold isn't met.\n",
    "        results.append(idDate == idDate)\n",
    "    \n",
    "    return pd.Series({'{}d'.format(winSizes[i]): results[i] for i in range(len(winSizes)) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleThreshold():\n",
    "    return tlSty.groupby('ClientId').progress_apply(SleepThreshChronicTest,threshPct=0.75,winSizes=predictionTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc62c107e8b45ebb35fc076cdd2b510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thshTest = SimpleThreshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpCls = { t: MLPClassifier(\n",
    "                activation='relu',\n",
    "                alpha=0.05,\n",
    "                hidden_layer_sizes=(100,),\n",
    "                learning_rate='adaptive',\n",
    "                solver='sgd') \n",
    "            for t in predictionTimes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrCls = { t: LogisticRegression() for t in predictionTimes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datXNrm = copy.deepcopy(datX)\n",
    "for colInd in datX.columns:\n",
    "    datXNrm[colInd] = (datX[colInd]-datX[colInd].mean())/np.sqrt(datX[colInd].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datY = tte.Flag == 'chr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Since the stratified K-fold routine randomly selects training and testing sets each time it's called, the results shown below may differ slightly from the results we show in our publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSplit = 10\n",
    "nRepeat = 1\n",
    "skf = RepeatedStratifiedKFold(n_splits=nSplit, n_repeats=nRepeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnPrf = { t: { 'TP': 0, 'FP': 0, 'TN': 0, 'CndP': 0, 'CndN': 0 } for t in predictionTimes }\n",
    "lrPrf = { t: { 'TP': 0, 'FP': 0, 'TN': 0, 'CndP': 0, 'CndN': 0 } for t in predictionTimes }\n",
    "thshPrf = { t: { 'TP': 0, 'FP': 0, 'TN': 0, 'CndP': 0, 'CndN': 0 } for t in predictionTimes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnCohort = { t: [] for t in predictionTimes }\n",
    "lrCohort = { t: [] for t in predictionTimes }\n",
    "thshCohort = { t: [] for t in predictionTimes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ThreshEval(res,threshResults,dataY,testInd,trainInd):\n",
    "    hat = threshResults.iloc[testInd]\n",
    "\n",
    "    res['TP'] += sum((hat == True) & (dataY.iloc[testInd] == True))\n",
    "    res['FP'] += sum((hat == True) & (dataY.iloc[testInd] == False))\n",
    "    res['TN'] += sum((hat == False) & (dataY.iloc[testInd] == False))\n",
    "    res['CndP'] += sum(dataY.iloc[testInd] == True)\n",
    "    res['CndN'] += sum(dataY.iloc[testInd] == False)\n",
    "    \n",
    "    return threshResults.iloc[testInd].loc[hat].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLTrainAndEval(est,res,dataX,dataY,testInd,trainInd):\n",
    "    est.fit(dataX.iloc[trainInd], dataY.iloc[trainInd])\n",
    "    hat = est.predict(dataX.iloc[testInd])\n",
    "    \n",
    "    res['TP'] += sum((hat == True) & (dataY.iloc[testInd] == True).to_numpy())\n",
    "    res['FP'] += sum((hat == True) & (dataY.iloc[testInd] == False).to_numpy())\n",
    "    res['TN'] += sum((hat == False) & (dataY.iloc[testInd] == False).to_numpy())\n",
    "    res['CndP'] += sum(dataY.iloc[testInd] == True)\n",
    "    res['CndN'] += sum(dataY.iloc[testInd] == False)\n",
    "    \n",
    "    return dataX.iloc[testInd].loc[hat].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Time: 90 days\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7ca2fa7911488ab11d06dc38858d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tPred in predictionTimes:\n",
    "    print('Prediction Time: {} days'.format(tPred))\n",
    "    tStr = '{}d'.format(tPred)\n",
    "    \n",
    "    for trainInd, testInd in tqdm(skf.split(datXNrm, datY),total=nSplit*nRepeat):\n",
    "\n",
    "        nnCohort[tPred].extend(\n",
    "            MLTrainAndEval(mlpCls[tPred],nnPrf[tPred],datXNrm[tStr],datY,testInd,trainInd))\n",
    "\n",
    "        lrCohort[tPred].extend(\n",
    "            MLTrainAndEval(lrCls[tPred],lrPrf[tPred],datXNrm[tStr][['Age','Sleep']],datY,testInd,trainInd))\n",
    "        \n",
    "        thshCohort[tPred].extend(\n",
    "            ThreshEval(thshPrf[tPred],thshTest[tStr],datY,testInd,trainInd))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDemographyStats(demog,cohortInd,nRepeat): \n",
    "    cohort = demog.loc[cohortInd]\n",
    "    \n",
    "    longFields = { 'TotalStays': 'Total Stays', 'TotalEpisodes': 'Total Episodes', 'Tenure': 'Tenure (days)', \n",
    "                  'UsagePct': 'Usage Percentage', 'AvgGapLen': 'Average Gap Length (days)' }\n",
    "    \n",
    "    nPop = len(demog.index)\n",
    "    nCohort = int(len(cohort.index)/nRepeat)\n",
    "    print( 'Avg clients in cohort: %d/%d (%.1f%%)' % (nCohort,nPop,100*nCohort/nPop))\n",
    "\n",
    "    fields = [ 'TotalStays', 'TotalEpisodes', 'Tenure', 'UsagePct', 'AvgGapLen' ]\n",
    "    for field in fields:\n",
    "        print('%s:' % (field))\n",
    "        nEntry = sum(~np.isnan(cohort[field]))                \n",
    "        print(' Avg: {:.1f}, Med: {:.1f}, 10thPct: {:.1f}, 90thPct: {:.1f}' \n",
    "              .format(cohort[field].mean(),cohort[field].median(),\n",
    "                    cohort[field].sort_values().iloc[int(nEntry*0.1)],\n",
    "                    cohort[field].sort_values().iloc[int(nEntry*0.9)]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintEstimatorPerformance(r):\n",
    "    print('TPR/Sens: %g (%d), FPR/FlsAlrm: %g (%d), Confidence: %g, Accuracy: %g' \n",
    "          % (r['TP']/r['CndP'], r['TP'], r['FP']/r['CndN'], r['FP'], r['TP']/(r['TP']+r['FP']), \n",
    "             (r['TP']+r['TN'])/(r['CndN']+r['CndP'])   ))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Prediction at 90 Days ----\n",
      "TPR/Sens: 0.316333 (490), FPR/FlsAlrm: 0.0162621 (274), Confidence: 0.641361, Accuracy: 0.927546\n",
      "\n",
      "Avg clients in cohort: 764/18398 (4.2%)\n",
      "TotalStays:\n",
      " Avg: 671.8, Med: 409.5, 10thPct: 113.0, 90thPct: 1687.0\n",
      "TotalEpisodes:\n",
      " Avg: 3.8, Med: 3.0, 10thPct: 1.0, 90thPct: 8.0\n",
      "Tenure:\n",
      " Avg: 1273.6, Med: 1055.0, 10thPct: 201.0, 90thPct: 2662.0\n",
      "UsagePct:\n",
      " Avg: 60.6, Med: 60.4, 10thPct: 13.7, 90thPct: 100.8\n",
      "AvgGapLen:\n",
      " Avg: 3.1, Med: 1.5, 10thPct: 0.9, 90thPct: 7.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tPred in predictionTimes:\n",
    "    print('---- Prediction at {} Days ----'.format(tPred))\n",
    "    PrintEstimatorPerformance(lrPrf[tPred])\n",
    "    PrintDemographyStats(demog,lrCohort[tPred],nRepeat)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Prediction at 90 Days ----\n",
      "TPR/Sens: 0.355713 (551), FPR/FlsAlrm: 0.0179833 (303), Confidence: 0.645199, Accuracy: 0.929286\n",
      "\n",
      "Avg clients in cohort: 854/18398 (4.6%)\n",
      "TotalStays:\n",
      " Avg: 660.7, Med: 394.5, 10thPct: 106.0, 90thPct: 1681.0\n",
      "TotalEpisodes:\n",
      " Avg: 3.7, Med: 3.0, 10thPct: 1.0, 90thPct: 8.0\n",
      "Tenure:\n",
      " Avg: 1295.4, Med: 1091.5, 10thPct: 174.0, 90thPct: 2702.0\n",
      "UsagePct:\n",
      " Avg: 59.1, Med: 59.9, 10thPct: 13.0, 90thPct: 99.9\n",
      "AvgGapLen:\n",
      " Avg: 3.3, Med: 1.6, 10thPct: 1.0, 90thPct: 7.7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tPred in predictionTimes:\n",
    "    print('---- Prediction at {} Days ----'.format(tPred))\n",
    "    PrintEstimatorPerformance(nnPrf[tPred])\n",
    "    PrintDemographyStats(demog,nnCohort[tPred],nRepeat)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance: Threshold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Prediction at 90 Days ----\n",
      "TPR/Sens: 0.985152 (1526), FPR/FlsAlrm: 0.0582824 (982), Confidence: 0.608453, Accuracy: 0.945374\n",
      "\n",
      "Avg clients in cohort: 2508/18398 (13.6%)\n",
      "TotalStays:\n",
      " Avg: 563.3, Med: 362.0, 10thPct: 120.0, 90thPct: 1271.0\n",
      "TotalEpisodes:\n",
      " Avg: 5.4, Med: 4.0, 10thPct: 1.0, 90thPct: 11.0\n",
      "Tenure:\n",
      " Avg: 1526.9, Med: 1397.0, 10thPct: 288.0, 90thPct: 2961.0\n",
      "UsagePct:\n",
      " Avg: 44.6, Med: 36.7, 10thPct: 11.1, 90thPct: 93.8\n",
      "AvgGapLen:\n",
      " Avg: 4.2, Med: 2.7, 10thPct: 1.0, 90thPct: 9.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tPred in predictionTimes:\n",
    "    print('---- Prediction at {} Days ----'.format(tPred))\n",
    "    PrintEstimatorPerformance(thshPrf[tPred])\n",
    "    PrintDemographyStats(demog,thshCohort[tPred],nRepeat)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
