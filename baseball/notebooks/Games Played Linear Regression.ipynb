{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Games Played Linear Regression.ipynb - Baseball demonstration of linar regression.\n",
    "#     Copyright (C) 2020  Geoffrey G. Messier\n",
    "# \n",
    "#     This program is free software: you can redistribute it and/or modify\n",
    "#     it under the terms of the GNU General Public License as published by\n",
    "#     the Free Software Foundation, either version 3 of the License, or\n",
    "#     (at your option) any later version.\n",
    "# \n",
    "#     This program is distributed in the hope that it will be useful,\n",
    "#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#     GNU General Public License for more details.\n",
    "# \n",
    "#     You should have received a copy of the GNU General Public License\n",
    "#     along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy as sci\n",
    "import scipy.special as scisp\n",
    "import scipy.stats as scist\n",
    "import datetime, copy, imp\n",
    "import re\n",
    "import sys\n",
    "import numexpr as ne\n",
    "import random\n",
    "\n",
    "\n",
    "import MySQLdb\n",
    "\n",
    "import pymysql.cursors;\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "tqdm.pandas()\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>GamesPlayed</th>\n",
       "      <th>AtBats</th>\n",
       "      <th>AvgOuts</th>\n",
       "      <th>AvgOnBase</th>\n",
       "      <th>AvgRuns</th>\n",
       "      <th>AvgRbis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PlayerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abrej003</th>\n",
       "      <td>FirstBase</td>\n",
       "      <td>129</td>\n",
       "      <td>549</td>\n",
       "      <td>7.139535</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.123862</td>\n",
       "      <td>0.078324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acunr001</th>\n",
       "      <td>LeftField</td>\n",
       "      <td>111</td>\n",
       "      <td>484</td>\n",
       "      <td>1.657658</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.070248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaml001</th>\n",
       "      <td>LeftField</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adamm002</th>\n",
       "      <td>FirstBase</td>\n",
       "      <td>117</td>\n",
       "      <td>332</td>\n",
       "      <td>3.658120</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.126506</td>\n",
       "      <td>0.108434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adamw002</th>\n",
       "      <td>ShortStop</td>\n",
       "      <td>85</td>\n",
       "      <td>320</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.134375</td>\n",
       "      <td>0.059375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zimmb001</th>\n",
       "      <td>CenterField</td>\n",
       "      <td>36</td>\n",
       "      <td>114</td>\n",
       "      <td>2.277778</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zimmj003</th>\n",
       "      <td>Pitcher</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zimmr001</th>\n",
       "      <td>FirstBase</td>\n",
       "      <td>85</td>\n",
       "      <td>318</td>\n",
       "      <td>6.541176</td>\n",
       "      <td>0.342767</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.103774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zobrb001</th>\n",
       "      <td>SecondBase</td>\n",
       "      <td>137</td>\n",
       "      <td>513</td>\n",
       "      <td>2.350365</td>\n",
       "      <td>0.382066</td>\n",
       "      <td>0.130604</td>\n",
       "      <td>0.070175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zunim001</th>\n",
       "      <td>Catcher</td>\n",
       "      <td>109</td>\n",
       "      <td>402</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.092040</td>\n",
       "      <td>0.049751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Position  GamesPlayed  AtBats   AvgOuts  AvgOnBase   AvgRuns  \\\n",
       "PlayerId                                                                    \n",
       "abrej003    FirstBase          129     549  7.139535   0.327869  0.123862   \n",
       "acunr001    LeftField          111     484  1.657658   0.367769  0.161157   \n",
       "adaml001    LeftField           24      28  0.375000   0.357143  0.357143   \n",
       "adamm002    FirstBase          117     332  3.658120   0.313253  0.126506   \n",
       "adamw002    ShortStop           85     320  2.882353   0.350000  0.134375   \n",
       "...               ...          ...     ...       ...        ...       ...   \n",
       "zimmb001  CenterField           36     114  2.277778   0.280702  0.122807   \n",
       "zimmj003      Pitcher            2       2  0.000000   0.000000  0.000000   \n",
       "zimmr001    FirstBase           85     318  6.541176   0.342767  0.103774   \n",
       "zobrb001   SecondBase          137     513  2.350365   0.382066  0.130604   \n",
       "zunim001      Catcher          109     402  0.192661   0.261194  0.092040   \n",
       "\n",
       "           AvgRbis  \n",
       "PlayerId            \n",
       "abrej003  0.078324  \n",
       "acunr001  0.070248  \n",
       "adaml001  0.142857  \n",
       "adamm002  0.108434  \n",
       "adamw002  0.059375  \n",
       "...            ...  \n",
       "zimmb001  0.052632  \n",
       "zimmj003  0.000000  \n",
       "zimmr001  0.103774  \n",
       "zobrb001  0.070175  \n",
       "zunim001  0.049751  \n",
       "\n",
       "[991 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = pd.read_hdf('~/data/baseball/PlayerSummary-2018.hdf',key='Data')\n",
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPlayers = len(players.index)\n",
    "pct70 = players.GamesPlayed.sort_values().iloc[int(0.3*nPlayers)]\n",
    "players70 = players[players.GamesPlayed >= pct70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory and Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let the $M \\times (P+1)$ input data matrix $\\mathbb{X} = [\\ \\mathbb{X}_0\\ \\mathbb{X}_1\\ \\ldots\\ \\mathbb{X}_p\\ ]$, where $M$ is the number of training samples, $P$ is the number of data features and $\\mathbb{X}_0$ is a column of 1's.\n",
    "- This makes the linear hypothesis function equal to $\\hat{\\mathbb{Y}} = \\mathbb{X}\\mathbb{\\beta}$.\n",
    "- The sum of squares function and its derivatives:\n",
    "\\begin{equation*}\n",
    "\\begin{array}{c}\n",
    "J(\\beta) = (\\mathbb{Y} - \\mathbb{X}\\beta)^T(\\mathbb{Y} - \\mathbb{X}\\beta) \\\\\n",
    "\\delta J/ \\delta\\beta = - 2\\mathbb{X}^T(\\mathbb{Y} - \\mathbb{X}\\beta) \\\\\n",
    "\\delta^2 J/ (\\delta\\beta\\delta\\beta^T) = 2\\mathbb{X}^T \\mathbb{X} \\\\\n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Least Squares Coefficient Calculation\n",
    "- The least square solution for the coefficients is found by setting the first derivative of $J(\\beta)$ to zero and solving for $\\beta$.  The result is $\\hat{\\beta} = \\left(\\mathbb{X}^T\\mathbb{X}\\right)^{-1}\\mathbb{X}^T\\mathbb{Y}$.\n",
    "- If this calculation is too expensive, iterative methods like gradient descent can be used to solve for the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(players70.index)\n",
    "p = 4\n",
    "\n",
    "# Target variable\n",
    "y = np.array([ players70.GamesPlayed ]).transpose()\n",
    "\n",
    "# Input variables (p=4 features)\n",
    "x = np.ones((m,p+1))\n",
    "x[:,1:] = players70[['AvgOuts','AvgOnBase','AvgRuns','AvgRbis']].to_numpy()\n",
    "#x[:,1:] = players70[['AvgOuts','AvgOnBase']].to_numpy()\n",
    "#x[:,1:] = players70[['AtBats']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrm = np.ones((1,p+1))\n",
    "nrm[0,1:] = np.sqrt(x[:,1:].var(axis=0))\n",
    "x = x/nrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-19.98833285],\n",
       "       [ 14.15212167],\n",
       "       [ 23.91125286],\n",
       "       [ -0.78502377],\n",
       "       [  2.51856959]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct least squares calculation\n",
    "betaLs = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "yHat = x @ betaLs\n",
    "betaLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarYHat:  1449.8866523323609\n",
      "Z-Scores:  [[ 4.09089264  8.69304716 11.55411656  0.46225475  1.46177651]]\n",
      "P-Values:  [[2.40150055e-05 0.00000000e+00 0.00000000e+00 3.22021973e-01\n",
      "  7.21282056e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Z-score calculation\n",
    "# - Scores greater than ~2 are significant at the 5% level.\n",
    "varYHat = (y-yHat).var()\n",
    "vs = np.linalg.inv( x.T @ x ).diagonal()\n",
    "zScoreLs = np.abs( betaLs.T / np.sqrt( varYHat * vs ) )\n",
    "degFreedom = m - p - 1\n",
    "pVals = 1-scist.t.cdf(zScoreLs,degFreedom)\n",
    "\n",
    "print('VarYHat: ', varYHat )\n",
    "print('Z-Scores: ', zScoreLs )\n",
    "print('P-Values: ', pVals )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that we should be able to drop the average number of runs and RBIs without significantly affecting model accuracy.  More thoughtful techniques for adjusting/removing coefficients include subset discovery, Lasso and LAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Order Models\n",
    "- Use the second order polynomial of each of the native data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(players70.index)\n",
    "p = 8\n",
    "\n",
    "# Target variable\n",
    "y = np.array([ players70.GamesPlayed ]).transpose()\n",
    "\n",
    "# Input variables (p=4 features)\n",
    "x = np.ones((m,p+1))\n",
    "x[:,1:5] = players70[['AvgOuts','AvgOnBase','AvgRuns','AvgRbis']].to_numpy()\n",
    "x[:,5:] = x[:,1:5]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrm = np.ones((1,p+1))\n",
    "nrm[0,1:] = np.sqrt(x[:,1:].var(axis=0))\n",
    "x = x/nrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.44442507],\n",
       "       [ 21.54661882],\n",
       "       [-16.61195531],\n",
       "       [  3.68803744],\n",
       "       [ 32.25878367],\n",
       "       [ -9.52877319],\n",
       "       [ 33.15456024],\n",
       "       [ -3.38022912],\n",
       "       [-28.82767128]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct least squares calculation\n",
    "betaLs = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "yHat = x @ betaLs\n",
    "betaLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarYHat:  1274.8341142447775\n",
      "Z-Scores:  [[1.10232615 5.54740004 2.63215836 1.18709085 7.75458762 2.65936355\n",
      "  5.61213515 1.27364884 7.68630026]]\n",
      "P-Values:  [[1.35352680e-01 2.06921361e-08 4.33735447e-03 1.17800748e-01\n",
      "  1.59872116e-14 4.00551816e-03 1.44939580e-08 1.01609006e-01\n",
      "  2.62012634e-14]]\n"
     ]
    }
   ],
   "source": [
    "# Z-score calculation\n",
    "# - Scores greater than ~2 are significant at the 5% level.\n",
    "varYHat = (y-yHat).var()\n",
    "vs = np.linalg.inv( x.T @ x ).diagonal()\n",
    "zScoreLs = np.abs( betaLs.T / np.sqrt( varYHat * vs ) )\n",
    "degFreedom = m - p - 1\n",
    "pVals = 1-scist.t.cdf(zScoreLs,degFreedom)\n",
    "\n",
    "print('VarYHat: ', varYHat )\n",
    "print('Z-Scores: ', zScoreLs )\n",
    "print('P-Values: ', pVals )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we now have a large number of statistically significant model parameters.  However, when correlation exists between features in the data, one feature is sometimes assigned a large positive coefficient and the other a large negative coefficient.  This occurs between lower an higher order terms on the same native data feature.  These parameters can sometimes cancel each other out without significantly affecting performance.  Ridge regression attempts to counter-act this by placing a restriction on the size of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Overfitting and Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training error is the sum of squares error between the model and the data used to train it.\n",
    "- Generalization error is the difference between the model and data outside of the training set.\n",
    "- As complexity is added, training error tends to go down.  However, there is a risk of the model overfitting the training set and resulting in an increase in generalization error.\n",
    "- We can balance this by *cross-validation* which trains on a subset of the data and evaluates that model performance based on generalization error calculated by the data outside of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvFrac = 0.3\n",
    "nIter = 20\n",
    "\n",
    "trainErr = np.zeros(nIter)\n",
    "genErr = np.zeros(nIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(players70.index)\n",
    "p = 4\n",
    "y = np.array([ players70.GamesPlayed ]).transpose()\n",
    "x = np.ones((m,p+1))\n",
    "x[:,1:] = players70[['AvgOuts','AvgOnBase','AvgRuns','AvgRbis']].to_numpy()\n",
    "\n",
    "nrm = np.ones((1,p+1))\n",
    "nrm[0,1:] = np.sqrt(x[:,1:].var(axis=0))\n",
    "x = x/nrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Train Var: 1436.61, CV Var: 1479.87\n",
      "Iteration 1 - Train Var: 1414.32, CV Var: 1563.22\n",
      "Iteration 2 - Train Var: 1432.63, CV Var: 1482.54\n",
      "Iteration 3 - Train Var: 1405.83, CV Var: 1578.9\n",
      "Iteration 4 - Train Var: 1426.9, CV Var: 1527.1\n",
      "Iteration 5 - Train Var: 1469.21, CV Var: 1412.81\n",
      "Iteration 6 - Train Var: 1451.38, CV Var: 1456.46\n",
      "Iteration 7 - Train Var: 1428.14, CV Var: 1505.67\n",
      "Iteration 8 - Train Var: 1469.05, CV Var: 1483.31\n",
      "Iteration 9 - Train Var: 1410.15, CV Var: 1558.23\n",
      "Iteration 10 - Train Var: 1353.06, CV Var: 1678.77\n",
      "Iteration 11 - Train Var: 1457.86, CV Var: 1444.35\n",
      "Iteration 12 - Train Var: 1394.07, CV Var: 1580.23\n",
      "Iteration 13 - Train Var: 1455.19, CV Var: 1448.25\n",
      "Iteration 14 - Train Var: 1429.94, CV Var: 1495.09\n",
      "Iteration 15 - Train Var: 1462.51, CV Var: 1422.57\n",
      "Iteration 16 - Train Var: 1386.18, CV Var: 1609.47\n",
      "Iteration 17 - Train Var: 1440.18, CV Var: 1499.24\n",
      "Iteration 18 - Train Var: 1409.58, CV Var: 1625.01\n",
      "Iteration 19 - Train Var: 1442.05, CV Var: 1475.61\n",
      "----\n",
      "Avg Train Var: 1428.74, Avg CV Var: 1516.33\n"
     ]
    }
   ],
   "source": [
    "mTrain = int(m*(1-cvFrac))\n",
    "xTrain = np.zeros((mTrain,p+1))\n",
    "xCv = np.zeros((m-mTrain,p+1))\n",
    "yTrain = np.zeros((mTrain,1))\n",
    "yCv = np.zeros((m-mTrain,1))\n",
    "\n",
    "varTrainAvg = 0\n",
    "varCvAvg = 0\n",
    "\n",
    "for iter in range(0,nIter):\n",
    "    \n",
    "    indTrain = random.sample(range(m),mTrain)\n",
    "    indCv = [ i for i in range(m) if i not in indTrain ]\n",
    "    \n",
    "    xTrain[:,:] = x[indTrain,:]\n",
    "    yTrain[:] = y[indTrain]\n",
    "    xCv[:,:] = x[indCv,:]\n",
    "    yCv[:] = y[indCv]\n",
    "    \n",
    "    beta = np.linalg.inv(xTrain.T @ xTrain) @ xTrain.T @ yTrain\n",
    "    \n",
    "    yHatTrain = xTrain @ beta\n",
    "    yHatCv = xCv @ beta\n",
    "    \n",
    "    varTrain = (yTrain - yHatTrain).var()\n",
    "    varCv = (yCv - yHatCv).var()\n",
    "    \n",
    "    print('Iteration %d - Train Var: %g, CV Var: %g' % (iter,varTrain,varCv))\n",
    "        \n",
    "    varTrainAvg += varTrain\n",
    "    varCvAvg += varCv\n",
    "    \n",
    "print('----')\n",
    "print('Avg Train Var: %g, Avg CV Var: %g' % (varTrainAvg/nIter,varCvAvg/nIter))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our training and generalization error is quite close.  In the contrived examples of overfitting, we typically see a picture of a small number of data points and a model that curves to meet each point.  In our baseball data, we have a large number independent data samples and a smaller number of features so this overfitting isn't really possible.  We might see this more if a large portion of the players were clumped into a small number of highly correlated groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- This section includes some notes related to Chapter 3 of [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf).\n",
    "\n",
    "#### Introduction\n",
    "- The training data can be represented by a $\\mathbb{X} \\in \\mathcal{R}^{M\\times P}$ matrix where $M$ is the number of training samples and $P$ is the number of features in the data.  The $m$th row is $\\mathbb{x}_m = [\\ x_{m1},\\ldots x_{mP}\\ ]$ and the $p$th column is $\\mathbb{X}_p$.\n",
    "- A note on the features:\n",
    "  - We have $N \\leq P$ actual features in the data.\n",
    "  - The remaining features can be created by taking higher order *basis expansions* of the original features.\n",
    "  - If we have qualitative data (ie. player position), we can include this data by \"dummy coding\" it where each value is represented by a different feature and each feature is equal to $\\{0,1\\}$ depending on if it is true or false.\n",
    "  \n",
    "- The hypothesis function is $h(\\mathbb{x}_m) = \\mathbb{\\beta} \\mathbb{x_m}^T$ where $\\mathbb{\\beta} = [\\ \\beta_0\\ \\ldots\\ \\beta_P\\ ]$.\n",
    "- The residual sum of squares, RSS$(\\mathbb{\\beta})$, is the squared difference between the hypothesis function output and the actual target variable values summed over the $M$ training samples.\n",
    "- We can solve for the parameters that minimize RSS directly using the pseudo-inverse (least squares projection) of the entire $\\mathbb{X}$ matrix.  This works even if $\\mathbb{X}$ isn't full rank (some of the columns are linearly dependent on others).\n",
    "- The matrix inverse can be avoided by minimizing RSS using gradient descent or Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Distribution & Model Statistics\n",
    "- Assume that $h(\\mathbb{X})$ is the true model for the mean of the target variable data $\\mathbb{Y}$ and that any deviation about that mean observed in the data is Gaussian distributed so that $\\mathbb{Y} = h(\\mathbb{X}) + \\epsilon$ and $\\epsilon$ is a Gaussian random variable.\n",
    "- The estimated model parameters are Gaussian distributed and the estimated target variable variance is Chi squared distributed.\n",
    "\n",
    "- These assumptions allow us to evaluate confidence in the model parameters.\n",
    "- The model parameters can be normalized by their standard deviations to create a Z-score to for use in a statistical confidence calculation:\n",
    "  - A good discussion of statistical inference and the null hypothesis can be found in Chapter 7 of [Campbell, et.al.](https://ucalgary-primo.hosted.exlibrisgroup.com/permalink/f/12m0b6n/01UCALG_ALMA51675036160004336).\n",
    "  - The *null hypothesis* is often used since it's easier to disprove.  For example, a hypothesis could be \"All Asian children have black hair.\" which is hard to prove since all Asian children would need to be observed.  However, the hypothesis is much easier to disprove.  The null hypothesis is \"There is at least one Asian child who does not have black hair.\" which is easier to prove since there is a chance we could prove this by observing just one child.\n",
    "  - Our hypothesis is that a data feature has influence over the target variable and its corresponding parameter is non-zero in order to reflect that influence.  The null hypothesis is that the parameter is equal to zero.\n",
    "  - No parameter will be truly equal to zero and our task is to determine whether a non-zero value of $\\beta$ is simply due to random variation.  \n",
    "  - A $p$-value corresponds to the probability that we discard a statistically significant model parameter as null.  Typically, $p < 0.05$.  We can use the t-distribution to determine the threshold below which random variations in a zero mean parameter would fall $1-p$ percent of the time.  \n",
    "  - If the Z-score for our parameter is larger than this threshold, we assume that the null hypothesis is false, that we can have confidence that our model parameter is statistically significant and that it should be retained in our model.\n",
    "- The F-statistic can be used to determine whether a group of parameters can be dropped from the model.  Useful for when a categorical variable is represented by a group of dummy indicator function variables.\n",
    "- 95% confidence intervals for each model parameter can also be calculated either individually or as a group (individual is probably best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gauss-Markov Theory\n",
    "- Start by assuming a linear combination of the parameters $\\mathbb{x}_0^T\\mathbb{\\beta}$.  In this case, the data vector $\\mathbb{x}_0$ is fixed.  Our calculation of the linear combination $\\hat{\\mathbb{\\theta}} = \\mathbb{x}_0^T\\hat{\\mathbb{\\beta}}$ is an approximation since we're working with estimated values of the real underlying parameters.\n",
    "- These parameter estimates are generated from $\\mathbb{X}$, which we also assume is fixed.\n",
    "- The model is unbiased since the mean of the estimate equals the true values.  The theorem says the pseudo-inverse solution to the estimated parameters gives the minimum variance unbiased estimator.\n",
    "\n",
    "- While the theorem says least squares has the minimum MSE of all unbiased estimators, there may be another estimator with lower MSE that *has bias*.\n",
    "- Bias can be caused by shrinking or zeroing out least squares coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Selection\n",
    "- This is where we shrink parameters and/or remove variables to improve prediction accuracy (lowering prediction error by introducing bias) and make the model easier to understand by focusing on the most important variables.\n",
    "- Subset selection retains only a subset of the variables.\n",
    "- Easy to understand but there are criticisms of this method as being too chunky and/or relying on inaccurate statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrinkage Methods\n",
    "- Adds an extra penalty in the minimization to reduce the sum of squares of the model coefficients.  Shrinks the coefficients towards zero and each other.\n",
    "- Tends to eliminate large negative coefficients balanced by large positive coefficients which tends to occur when input variables are correlated.\n",
    "- Inputs need to be standardized (normalize to unit variance) and centered (means subtracted off).  The y-intercept parameter ($\\beta_0$) is not included in the penalty and is estimated by setting it equal to the mean of the target variable.\n",
    "- The model parameters can again be solved directly using the least squares solution (and likely gradient descent or Newton's method).\n",
    "- An SVD analysis demonstrates that ridge regression tends to shrink components that do not correspond to a large amount of variance in the target variable.  Likely because there is not a lot of statistical correlation.\n",
    "- Lasso has the nice feature that it keeps some variables right at zero but requires a quadratic programming solution.\n",
    "- LAR is a simpler approach that results in almost the same performance as Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
